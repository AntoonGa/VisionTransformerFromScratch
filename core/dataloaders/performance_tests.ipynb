{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be1688708832bd0d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Folder '../../datasets/mnist/train' already exists.\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:11, 42.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11.250001668930054\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:07, 58.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7.998485326766968\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:07, 61.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7.68399977684021\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:07, 60.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7.75200080871582\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:07, 61.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7.625999212265015\n",
      "42.310486793518066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from core.dataloaders.mnist import MnistDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "train_data_dir = r\"../../datasets/mnist/train\"\n",
    "\n",
    "\n",
    "# setting up transformation\n",
    "transformation = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # 1 channel to 3 channel requiered for this model\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "type(transformation)\n",
    "# creating dataset\n",
    "train_dataset = MnistDataset(\n",
    "    data_dir=train_data_dir,\n",
    "    train = True,\n",
    "    transform=transformation\n",
    ")\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "workers = 6\n",
    "prefetch = workers * 8\n",
    "batch_size = 32*4\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=workers,\n",
    "                              prefetch_factor=prefetch,\n",
    "                              persistent_workers=True,\n",
    "                              pin_memory=False)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    start_epoch = time.time()\n",
    "    for i, (images, labels) in tqdm(enumerate(train_dataloader)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    print(\"epoch\", time.time() - start_epoch)\n",
    "\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-05T15:05:55.823552600Z",
     "start_time": "2024-01-05T15:05:06.990140700Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# playing cards"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3574c9b4b77e8c3e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:02, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25.773422956466675\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:01, 32.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.8652148246765137\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:01, 32.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.8728489875793457\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:01, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.8766999244689941\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:01, 32.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.8691062927246094\n",
      "workers 8\n",
      "prefetch 8\n",
      "33.25829219818115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from core.dataloaders.playing_cards import PlayingCardDataset\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "train_data_dir = r\"../../datasets/playing_cards/train\"\n",
    "\n",
    "\n",
    "size = (224, 224)\n",
    "# setting up transformation\n",
    "# no augmentation for validation and test\n",
    "transformation = transforms.Compose([\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Resize(size)\n",
    "])\n",
    "\n",
    "# augmentation for training\n",
    "augmentation = transforms.Compose([\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Resize(size),\n",
    "        # we can use random choice to apply random transformations, identify when using raw image\n",
    "        # flip/rotations transforms\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomVerticalFlip(p=1),\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomRotation(degrees=45),\n",
    "            torch.nn.Identity(),            \n",
    "        ]),\n",
    "        # affine transforms\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees = 0, translate = (0., 0.2)),\n",
    "            torch.nn.Identity(),            \n",
    "        ]),\n",
    "        # color transforms\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "            transforms.RandomEqualize(p=1),\n",
    "            transforms.RandomPosterize(bits=2, p=1),\n",
    "            transforms.RandomInvert(p=1),\n",
    "            torch.nn.Identity(),            \n",
    "        ]),\n",
    "\n",
    "])\n",
    "# creating dataset\n",
    "train_dataset = PlayingCardDataset(\n",
    "    data_dir=train_data_dir,\n",
    "    transform=transformation\n",
    ")\n",
    "\n",
    "workers = 8\n",
    "prefetch =  8\n",
    "batch_size = 32*4\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=workers,\n",
    "                              prefetch_factor=prefetch,\n",
    "                              persistent_workers=True,\n",
    "                              pin_memory=False)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(5):\n",
    "    print(epoch)\n",
    "    start_epoch = time.time()\n",
    "    for i, (images, labels) in tqdm(enumerate(train_dataloader)):\n",
    "        pass\n",
    "    print(\"epoch\", time.time() - start_epoch)\n",
    "print(\"workers\", workers)\n",
    "print(\"prefetch\", prefetch)\n",
    "print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T16:22:50.426553Z",
     "start_time": "2024-01-05T16:22:15.968379700Z"
    }
   },
   "id": "e880266e724c7f79",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# playing cards hdf5 : Inconsistent results, hdf5 is slower or on par with ImageFolder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c472f7cff77fa1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"D:\\Dropbox\\Talan\\Codes\\VisionTransformerFromScratch\\core\\dataloaders\\playing_cards_hdf5.py\", line 30, in __getitem__\n    images = self.transform(db[\"images\"][index, :, :, :])\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_container.py\", line 53, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_container.py\", line 155, in forward\n    return transform(*inputs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_transform.py\", line 46, in forward\n    params = self._get_params(\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_geometry.py\", line 737, in _get_params\n    height, width = query_size(flat_inputs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_utils.py\", line 194, in query_size\n    raise TypeError(\"No image, video, mask or bounding box was found in the sample\")\nTypeError: No image, video, mask or bounding box was found in the sample\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 76\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28mprint\u001B[39m(epoch)\n\u001B[0;32m     75\u001B[0m start_epoch \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (images, labels) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(train_dataloader)):\n\u001B[0;32m     77\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     78\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\tqdm\\std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1175\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1179\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1180\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1181\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1343\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1344\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1369\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1371\u001B[0m     data\u001B[38;5;241m.\u001B[39mreraise()\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\_utils.py:694\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    691\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    693\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 694\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mTypeError\u001B[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"D:\\Dropbox\\Talan\\Codes\\VisionTransformerFromScratch\\core\\dataloaders\\playing_cards_hdf5.py\", line 30, in __getitem__\n    images = self.transform(db[\"images\"][index, :, :, :])\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_container.py\", line 53, in forward\n    outputs = transform(*inputs)\n              ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_container.py\", line 155, in forward\n    return transform(*inputs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_transform.py\", line 46, in forward\n    params = self._get_params(\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_geometry.py\", line 737, in _get_params\n    height, width = query_size(flat_inputs)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\agarc\\anaconda3\\envs\\ViT_py311\\Lib\\site-packages\\torchvision\\transforms\\v2\\_utils.py\", line 194, in query_size\n    raise TypeError(\"No image, video, mask or bounding box was found in the sample\")\nTypeError: No image, video, mask or bounding box was found in the sample\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# \n",
    "# from torch.utils.data import DataLoader\n",
    "# from core.dataloaders.playing_cards_hdf5 import PlayingCardDataset\n",
    "# import torchvision.transforms.v2 as transforms\n",
    "# \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.synchronize()\n",
    "# \n",
    "# train_data_dir = r\"../../datasets/playing_cards/train.hdf5\"\n",
    "# \n",
    "# \n",
    "# size = (224, 224)\n",
    "# # setting up transformation\n",
    "# # no augmentation for validation and test\n",
    "# transformation = transforms.Compose([\n",
    "#         # transforms.ToImage(),\n",
    "#         transforms.ToDtype(torch.float32, scale=True),\n",
    "#         transforms.Resize(size)\n",
    "# ])\n",
    "# \n",
    "# # augmentation for training\n",
    "# augmentation = transforms.Compose([\n",
    "#         # transforms.ToImage(),\n",
    "#         transforms.ToDtype(torch.float32, scale=True),\n",
    "#         transforms.Resize(size),\n",
    "#         # we can use random choice to apply random transformations, identify when using raw image\n",
    "#         # flip/rotations transforms\n",
    "#         transforms.RandomChoice([\n",
    "#             transforms.RandomVerticalFlip(p=1),\n",
    "#             transforms.RandomHorizontalFlip(p=1),\n",
    "#             transforms.RandomRotation(degrees=45),\n",
    "#             torch.nn.Identity(),            \n",
    "#         ]),\n",
    "#         # affine transforms\n",
    "#         transforms.RandomChoice([\n",
    "#             transforms.RandomAffine(degrees = 0, translate = (0., 0.2)),\n",
    "#             torch.nn.Identity(),            \n",
    "#         ]),\n",
    "#         # color transforms\n",
    "#         transforms.RandomChoice([\n",
    "#             transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "#             transforms.RandomEqualize(p=1),\n",
    "#             transforms.RandomPosterize(bits=2, p=1),\n",
    "#             transforms.RandomInvert(p=1),\n",
    "#             torch.nn.Identity(),            \n",
    "#         ]),\n",
    "# \n",
    "# ])\n",
    "# # creating dataset\n",
    "# train_dataset = PlayingCardDataset(\n",
    "#     in_file=train_data_dir,\n",
    "#     transform=augmentation\n",
    "# )\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# workers = 6\n",
    "# prefetch = workers * 8\n",
    "# batch_size = 32*4\n",
    "# train_dataloader = DataLoader(train_dataset,\n",
    "#                               batch_size=batch_size,\n",
    "#                               shuffle=True,\n",
    "#                               num_workers=workers,\n",
    "#                               prefetch_factor=prefetch,\n",
    "#                               persistent_workers=True,\n",
    "#                               pin_memory=False)\n",
    "# \n",
    "# \n",
    "# start = time.time()\n",
    "# for epoch in range(5):\n",
    "#     print(epoch)\n",
    "#     start_epoch = time.time()\n",
    "#     for i, (images, labels) in tqdm(enumerate(train_dataloader)):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#     print(\"epoch\", time.time() - start_epoch)\n",
    "# \n",
    "# print(time.time() - start)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T15:14:07.785099Z",
     "start_time": "2024-01-05T15:14:01.343468300Z"
    }
   },
   "id": "9ba185e6c169197f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T14:58:25.427895Z",
     "start_time": "2024-01-05T14:58:25.418347Z"
    }
   },
   "id": "f0e368f47a3a1321",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
